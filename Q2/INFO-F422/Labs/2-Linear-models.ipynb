{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Jacopo De Stefani - __[Jacopo.De.Stefani@ulb.ac.be](mailto:Jacopo.De.Stefani@ulb.ac.be)__\n",
    "### Théo Verhelst - __[Theo.Verhelst@ulb.ac.be](mailto:Theo.Verhelst@ulb.ac.be)__\n",
    "\n",
    "## TP 2 - Linear Models\n",
    "\n",
    "####  March 09, 2021\n",
    "\n",
    "#### Materials originally developed by *Yann-Aël Le Borgne, Fabrizio Carcillo and Gianluca Bontempi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "The process of supervised learning involves the presence of an entity (the learner, also called prediction model), whose goal is to learn the mapping between inputs and outputs in a given problem.\n",
    "\n",
    "A supervised learning problem can formulated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    " y = m(\\mathbf{x})  \n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "- $y$ represents the output variable (also called target)\n",
    "- $\\mathbf{x}$ represents the vector of inputs (also called features).\n",
    "- $m$ is the (unknown) mapping between input and outputs.\n",
    "\n",
    "In the majority of the supervised learning problems, the mapping $m$ between input and outputs is unknown and needs to be estimated on basis of the available input/output observation pairs $(\\mathbf{x}_i,y_i)$.\n",
    "\n",
    "In this practical, in order to better understand the estimation process, we will tackle a simple case, where, for the sake of clarity, we limit ourselves to one input variable, and we assume to have a linear (known) mapping between the variables.\n",
    "\n",
    "### Mathematical formulation\n",
    "\n",
    "Assume to have $N$ observation pairs $D_N =\\{(x_i,y_i)\\}$ generated by the following stochastic process:\n",
    "\n",
    "\\begin{equation*}\n",
    " y_i=\\beta_0+\\beta_1 x_i +w_i,\n",
    "\\end{equation*}\n",
    "\n",
    "where the $w_i$ are iid realisations of a random variable $\\mathbf{w}$ with null mean ($\\mu_{\\mathbf{w}}=0$)  and constant variance $\\sigma^2_{\\mathbf{w}}$.\n",
    "\n",
    "The $x_i$ can be seen as *fixed*, the only random component in the sample set $D_N$ is therefore contained in the $y_i$ (which are random due to the $w_i$).\n",
    "\n",
    "\n",
    "### Estimation using the mean square error\n",
    "\n",
    "The coefficients $\\beta_0$ and $\\beta_1$ can be estimated using the least squares method. This method consists of taking those estimators $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ which minimize\n",
    "\n",
    "\\begin{equation}\n",
    " R_{emp}=\\sum_{i=1}^N (y_i-\\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1} x_i.\n",
    "\\end{equation}\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "\\begin{equation}\n",
    " \\{\\hat{\\beta_0},\\hat{\\beta_1}\\}=\\arg\\min_{b_0,b_1}\\sum_{i=1}^N (y_i-b_0-b_1x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The solution is given by *(Eq. 1)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}},\\quad \\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x},  \n",
    "\\end{equation}\n",
    "\n",
    "Also, the $\\beta_1$ coefficient can be rewritten to highlight his dependency on the empirical correlation coefficient $\\hat{\\rho_{xy}}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}} = \\sqrt{\\frac{S_{yy}}{S_{xx}}}*\\hat{\\rho_{xy}}\n",
    "\\end{equation}\n",
    "\n",
    "where *(Eq. 2)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\bar{x}=\\frac{\\sum_{i=1}^Nx_i}{N},\\quad\\bar{y}=\\frac{\\sum_{i=1}^Ny_i}{N},\\quad S_{xy}=\\sum_{i=1}^N (x_i-\\bar{x})y_i,\\quad S_{xx}=\\sum_{i=1}^N(x_i-\\bar{x})^2.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "### Properties of the estimator\n",
    "\n",
    "* $E_{D_N}[\\hat{\\beta_1}]=\\beta_1$\n",
    "* $Var[\\hat{\\beta_1}]=\\frac{\\sigma^2}{S_{xx}}$\n",
    "* $E[\\hat{\\beta_0}]=\\beta_0$\n",
    "* $Var[\\hat{\\beta_0}]=\\sigma^2\\left( \\frac{1}{N}+\\frac{\\bar{x}^2}{S_{xx}}\\right)$\n",
    "* $\\hat{\\sigma}^2_{\\mathbf{w}}=\\frac{\\sum_{i=1}^N(y_i-\\hat{y_i})^2}{N-2}$ is a non-biased estimator of $\\sigma^2_{\\mathbf{w}}$.\n",
    "\n",
    "\n",
    "### Partitioning the variability\n",
    "\n",
    "The variability of the response $y_i$ can be expressed as follows\n",
    "\n",
    "\\begin{equation}\n",
    " \\sum_{i=1}^N(y_i-\\bar{y})^2=\\sum_{i=1}^N(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^N(y_i-\\hat{y_i})^2,\n",
    "\\end{equation}\n",
    "\n",
    "that is\n",
    "\n",
    "\\begin{equation}\n",
    " SS_{tot}=SS_{mod}+SS_{res}.\n",
    "\\end{equation}\n",
    "\n",
    "### The F-test\n",
    "\n",
    "Goal: test if the variable $y$ is really influenced by the variable $x$. This can be formulated as a hypothesis test $\\beta_1=0$. If the test is rejected, it can be deduced that $x$ influences $y$ significantly.\n",
    "\n",
    "It can be shown that given a normally distributed $\\mathbf{w}$: *(Eq. 3)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{SS_{mod}}{SS_{res}/(N-2)} \\sim F_{1,N-2}\n",
    "\\end{equation}\n",
    "\n",
    "if the hypothesis $\\beta_1=0$ is true.\n",
    "\n",
    "### The t-test\n",
    "\n",
    "It can be shown that given a normally distributed $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\beta_1}\\sim \\mathcal{N}(\\beta_1,\\sigma^2 / S_{xx})\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{\\hat{\\beta_1}-\\beta_1}{\\hat{\\sigma}}\\sqrt{S_{xx}} \\sim \\mathcal{T}_{N-2}.\n",
    "\\end{equation}\n",
    "\n",
    "This can be used for testing the following hypothesis: $\\hat{\\beta_1}=\\beta_1$.\n",
    "\n",
    "### Confidence intervals\n",
    "\n",
    "With a probability $1-\\alpha$, the true parameter $\\beta_1$ lies in the interval *(Eq. 4)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\beta_1}\\pm t_{\\alpha/2,N-2}\\cdot \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}.\n",
    "\\end{equation}\n",
    "\n",
    "### Variance of the response\n",
    "\n",
    "Let\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\mathbf{y}}(x)=\\hat{\\beta_0}+\\hat{\\beta_1} x.\n",
    "\\end{equation}\n",
    "\n",
    "We can show that for all $x$:\n",
    "\n",
    "\\begin{equation}\n",
    " E_{D_N}[ \\hat{\\mathbf{y}}(x)]=E_{[y]}[ {\\mathbf{y}}(x)]\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    " Var[ \\hat{\\mathbf{y}}(x)]=\\sigma^2\\left[\\frac{1}{N}+\\frac{(x-\\bar{x})^2}{S_{xx}}\\right].\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression exercises\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Compare with the theoretical part of this course (from slide 15 of the chapter [Regression Modelling](https://uv.ulb.ac.be/pluginfile.php/1728621/mod_resource/content/5/linregr.pdf)). \n",
    "\n",
    "The goal of this exercise is to investigate the link between two variables originating from medical data by studying the *ventricular shortening velocity* in function of *blood glucose*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "library(ISwR)\n",
    "data(thuesen)\n",
    "\n",
    "I <-! is.na(thuesen[,\"short.velocity\"])\n",
    "Y<-thuesen[I,\"short.velocity\"]\n",
    "X<-thuesen[I,\"blood.glucose\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Apply the mean square method \"by hand\" using equations (Eq. 1) and (Eq. 2) to compute the coefficients $\\beta_0$ and $\\beta_1$ of a linear model for our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Test the hypothesis $\\beta_1=0$ using an F-test using equation *(Eq. 3)* and the F distribution function `pf` followed by a t-test using equation *(Eq. 4)* and the t distribution function `pt`. Note that F-test is used to assess if the whole model is relevant. In the considered case (univariate regression) the model is defined by a single parameter ($\\beta_1$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Compute the confidence interval for $\\beta_1$ using equation (Eq. 4) and the function `qt`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*  Use the function `lm`(Y$^{\\sim}$X) to obtain the same results automatically and compare these with the ones obtained earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Visualize the data and the regression line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The goal of this exercise is to experimentally study the bias and the variance of $\\hat{\\beta_0}$, $\\hat{\\beta_1}$, $\\hat{\\sigma}$ and $\\hat{\\mathbf{y}}(x_i)$. See also the theoretical part of this course (slide 24 of the chapter [Regression Modelling](https://uv.ulb.ac.be/pluginfile.php/1728621/mod_resource/content/5/linregr.pdf)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "X<-seq(-10,10,by=1) # the x_i are fixed\n",
    "beta0<--1 # y_i = -1 + x_i + Normal(0,5)\n",
    "beta1<-1\n",
    "sd.w<-5\n",
    "N<-length(X)\n",
    "R<-100#00 # number of iterations for the simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "beta.hat.1<-numeric(R)\n",
    "beta.hat.0<-numeric(R)\n",
    "var.hat.w<-numeric(R)\n",
    "Y.hat<-array(NA,c(R,N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Compute $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ and $\\hat{\\sigma}$ and plot their distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Illustrate the theorem $Var[y(x_0)]=\\sigma^2\\left(\\frac{1}{N}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple regression exercise\n",
    "\n",
    "This example is taken from the theoretical part of this course (slide 27 of the chapter [Regression Modelling](https://uv.ulb.ac.be/pluginfile.php/1257537/mod_resource/content/4/linregr.pdf)).\n",
    "\n",
    "Mutiple linear dependence occurs when the variable $x$ is a vector instead of a scalar. The goal of this exercise is to verify the theoretical results for the estimators $\\hat{\\mathbf{\\sigma}}^2$ and $\\hat{\\mathbf{\\beta} }$ obtained for the least squares method (no bias and analytical results concerning $Var[\\hat{\\mathbf{\\beta}}]$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "library(MASS)\n",
    "\n",
    "# initial values for n, (sigma_w) and beta\n",
    "n<-3 # number of input variables\n",
    "p<-n+1\n",
    "beta<-seq(2,p+1) # beta =(2,3,...,n+2)\n",
    "sd.w<-5  \n",
    "\n",
    "# generating data D_N\n",
    "N<-100 # number of samples\n",
    "X<-array(runif(N*n,min=-20,max=20),c(N,n))\n",
    "X<-cbind(array(1,c(N,1)),X)\n",
    "\n",
    "\n",
    "R<-100#00 # number of iterations\n",
    "beta.hat<-array(0,c(p,R))\n",
    "var.hat.w<-numeric(R)\n",
    "Y.hat<-array(NA,c(R,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute $\\hat{Y}$, $\\hat{\\beta}$ and $\\hat{\\sigma}$ following the equations in the course slides 35 and 36.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the histograms for $\\hat{\\sigma}$ and for each $\\hat{\\beta}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
